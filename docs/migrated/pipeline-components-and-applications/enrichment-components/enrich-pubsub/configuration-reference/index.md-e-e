---
title: "Configuration"
date: "2021-08-14"
sidebar_position: 50
---

## enrich-pubsub

A minimal configuration file can be found on the [Github repo](https://github.com/snowplow/enrich/blob/master/config/config.pubsub.minimal.hocon), as well as a [comprehensive one](https://github.com/snowplow/enrich/blob/master/config/config.pubsub.extended.hocon).

<table class="has-fixed-layout"><tbody><tr><td><code>input.subscription</code></td><td>Required. PubSub subscription identifier for the collector payloads.<br/>Example: <code>projects/example-project/subscriptions/collectorPayloads</code></td></tr><tr><td><code>input.parallelPullCount</code></td><td>Optional. Number of threads used internally by permutive library to handle incoming messages. These threads do very little "work" apart from writing the message to a concurrent Queue.<br/>Default: <code>1</code></td></tr><tr><td><code>input.maxQueueSize</code></td><td>Optional. Configures the "max outstanding element count" of PubSub. This is the principal way we control concurrency in the app; it puts an upper bound on the number of events in memory at once. An event counts towards this limit starting from when it received by the permutive library, until we ack it (after publishing to output). The value must be large enough that it does not cause the sink to block whilst it is waiting for a batch to be completed.<br/>Default: <code>3000</code></td></tr><tr><td><code>output.good.topic</code></td><td>Required. Name of the PubSub topic that will receive the enriched events.<br/>Example: <code>projects/example-project/topics/enriched</code></td></tr><tr><td><code>output.good.attributes</code></td><td>Optional. Enriched event fields to add as <a href="https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage">PubSub message attributes</a>. For example, if this is <code>[ "app_id" ]</code> then the enriched event's <code>app_id</code> field will be an attribute of the PubSub message, as well as being a field within the message data</td></tr><tr><td><code>output.good.delayThreshold</code></td><td>Optional. Delay threshold to use for batching. After this amount of time has elapsed, before maxBatchSize and maxBatchBytes have been reached, messages from the buffer will be sent.<br/>Default: <code>200 milliseconds</code></td></tr><tr><td><code>output.good.maxBatchSize</code></td><td>Optional. Maximum number of messages sent within a batch. When the buffer reaches this number of messages they are sent.<br/>Default: <code>1000</code> (PubSub maximum)</td></tr><tr><td><code>output.good.maxBatchBytes</code></td><td>Optional. Maximum number of bytes sent within a batch. When the buffer reaches this size messages are sent.<br/>Default: <code>8000000</code> (PubSub maximum is 10MB)</td></tr><tr><td><code>output.pii.topic</code></td><td>Optional. Should be used in conjunction with the <a href="https://docs.snowplowanalytics.com/docs/enriching-your-data/available-enrichments/pii-pseudonymization-enrichment/">PII pseudonymization enrichment</a>. When configured, enables an extra output topic for writing a <a href="https://github.com/snowplow/iglu-central/blob/master/schemas/com.snowplowanalytics.snowplow/pii_transformation/jsonschema/1-0-0">pii_transformation event</a>.<br/>Example: <code>projects/test-project/topics/pii</code></td></tr><tr><td><code>output.pii.attributes</code></td><td>Same as <code>output.good.attributes</code> for pii events</td></tr><tr><td><code>output.pii.delayThreshold</code></td><td>Same as <code>output.good.delayThreshold</code> for pii events</td></tr><tr><td><code><code>output.pii.maxBatchSize</code></code></td><td>Same as <code>output.good.maxBatchSize</code> for pii events</td></tr><tr><td><code>output.pii.maxBatchBytes</code></td><td>Same as <code>output.good.maxBatchBytes</code> for pii events</td></tr><tr><td><code>output.bad.topic</code></td><td>Required. Name of the PubSub topic that will receive the bad rows.<br/>Example: <code>projects/example-project/topics/badrows</code></td></tr><tr><td><code>output.bad.delayThreshold</code></td><td>Same as <code>output.good.delayThreshold</code> for bad rows</td></tr><tr><td><code>output.bad.maxBatchSize</code></td><td>Same as <code>output.good.maxBatchSize</code> for bad rows</td></tr><tr><td><code>output.bad.maxBatchBytes</code></td><td>Same as <code>output.good.maxBatchBytes</code> for bad rows</td></tr></tbody></table>

## enrich-kinesis

A minimal configuration file can be found on the [Github repo](https://github.com/snowplow/enrich/blob/master/config/config.kinesis.minimal.hocon), as well as a [comprehensive one](https://github.com/snowplow/enrich/blob/master/config/config.kinesis.extended.hocon).

<table class="has-fixed-layout"><tbody><tr><td><code>input.appName</code></td><td>Optional. Name of the application which the KCL daemon should assume. A DynamoDB table with this name will be created.<br/>Default: <code>snowplow-enrich-kinesis</code></td></tr><tr><td><code>input.streamName</code></td><td>Required. Name of the Kinesis stream with the collector payloads to read from</td></tr><tr><td><code>input.region</code></td><td>Optional. Region where the Kinesis stream is located. This field is optional if it can be resolved with <a href="https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/regions/providers/DefaultAwsRegionProviderChain.html">AWS region provider chain</a>. It checks places like env variables, system properties, AWS profile file.</td></tr><tr><td><code>input.initialPosition.type</code></td><td>Optional. Set the initial position to consume the Kinesis stream. Possible values:<br/>- <code>LATEST</code>: most recent data<br/>- <code>TRIM_HORIZON</code>: oldest available data<br/>- <code>AT_TIMESTAMP</code>: start from the record at or after the specified timestamp<br/>Default: <code>TRIM_HORIZON</code></td></tr><tr><td><code>input.initialPosition.timestamp</code></td><td>Required for <code>AT_TIMESTAMP</code>.<br/>Example: <code>2020-07-17T10:00:00Z</code></td></tr><tr><td><code>input.retrievalMode.type</code></td><td>Optional. Set the mode for retrieving records. Possible values:<br/>- <code>Polling</code><br/>- <code><a href="https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/">FanOut</a></code><br/>Default: <code>Polling</code></td></tr><tr><td><code>input.bufferSize</code></td><td>Optional. Size of the internal buffer used when reading messages from Kinesis, each buffer holding up to maxRecords from above.<br/>Default: <code>3</code></td></tr><tr><td><code>input.customEndpoint</code></td><td>Optional. Endpoint url configuration to override aws kinesis endpoints. Can be used to specify local endpoint when using localstack<br/>Example: <code>http://localhost:4566</code></td></tr><tr><td><code>input.dynamodbCustomEndpoint</code></td><td>Optional. Endpoint url configuration to override aws dyanomdb endpoint for Kinesis checkpoints lease table. Can be used to specify local endpoint when using localstack.<br/>Example: <code>http://localhost:4569</code></td></tr><tr><td><code>input.cloudwatchCustomEndpoint</code></td><td>Optional. Endpoint url configuration to override aws cloudwatch endpoint for metrics. Can be used to specify local endpoint when using localstack.<br/>Example: <code>http://localhost:4582</code></td></tr><tr><td><code>input.retrievalMode.maxRecords</code></td><td>Required for <code>Polling</code>. Maximum size of a batch returned by a call to getRecords. Records are checkpointed after a batch has been fully processed, thus the smaller maxRecords, the more often records can be checkpointed into DynamoDb, but possibly reducing the throughput.<br/>Default: <code>10000</code></td></tr><tr><td><code>output.good.streamName</code></td><td>Required. Name of the Kinesis stream to write to the enriched events.<br/>Example: <code>enriched</code></td></tr><tr><td><code>output.good.region</code></td><td>Same as <code>input.region</code> for enriched events stream</td></tr><tr><td><code>output.good.partitionKey</code></td><td>Optional. How the output stream will be partitioned in Kinesis. Events with the same partition key value will go to the same shard.<br/>Possible partition keys: <code>event_id</code>, <code>event_fingerprint</code>, <code>domain_userid</code>, <code>network_userid</code>, <code>user_ipaddress</code>, <code>domain_sessionid</code>, <code>user_fingerprint</code><br/>Refer to <a href="https://github.com/snowplow/snowplow/wiki/canonical-event-model">this page</a> to know what the possible partition keys correspond to.<br/>If not specified, the partition key will be a random UUID</td></tr><tr><td><code>output.good.backoffPolicy.minBackoff</code></td><td>Optional. Minimum backoff before retrying when writing fails.<br/>Default: <code>100 milliseconds</code></td></tr><tr><td><code>output.good.backoffPolicy.maxBackoff</code></td><td>Optional. Maximum backoff before retrying when writing fails.<br/>Default: <code>10 seconds</code></td></tr><tr><td><code>output.good.backoffPolicy.maxRetries</code></td><td>Optional. Maximum number of retries.<br/>Default: <code>10</code></td></tr><tr><td><code>output.good.recordLimit</code></td><td>Optional. Limits the number of events in a single PutRecords request. Several requests are made in parallel.<br/>Default: 500</td></tr><tr><td><code>output.good.customEndpoint</code></td><td>Optional. To use a custom Kinesis endpoint.<br/>Example: <code>https://localhost:4566</code></td></tr><tr><td><code>output.pii.streamName</code></td><td>Optional. Should be used in conjunction with the <a href="https://docs.snowplowanalytics.com/docs/enriching-your-data/available-enrichments/pii-pseudonymization-enrichment/">PII pseudonymization enrichment</a>. When configured, enables an extra output stream for writing a <a href="https://github.com/snowplow/iglu-central/blob/master/schemas/com.snowplowanalytics.snowplow/pii_transformation/jsonschema/1-0-0">pii_transformation event</a>.<br/>Example: <code>pii</code></td></tr><tr><td><code>output.pii.region</code></td><td>Same as <code>output.good.region</code> for pii events</td></tr><tr><td><code>output.pii.partitionKey</code></td><td>Same as <code>output.good.partitionKey</code> for pii events</td></tr><tr><td><code>output.pii.backoffPolicy.minBackoff</code></td><td>Same as <code>output.good.backoffPolicy.minBackoff</code> for pii events</td></tr><tr><td><code>output.pii.backoffPolicy.maxBackoff</code></td><td>Same as <code>output.good.backoffPolicy.maxBackoff</code> for pii events</td></tr><tr><td><code>output.pii.backoffPolicy.maxRetries</code></td><td>Same as <code>output.good.backoffPolicy.maxRetries</code> for pii events</td></tr><tr><td><code>output.pii.recordLimit</code></td><td>Same as <code>output.good.recordLimit</code> for pii events</td></tr><tr><td><code>output.pii.customEndpoint</code></td><td>Same as <code>output.good.customEndpoint</code> for pii events</td></tr><tr><td><code>output.bad.streamName</code></td><td>Required. Name of the Kinesis stream to write to the bad rows.<br/>Example: <code>bad</code></td></tr><tr><td><code>output.bad.region</code></td><td>Same as <code>output.good.region</code> for bad rows</td></tr><tr><td><code>output.bad.backoffPolicy.minBackoff</code></td><td>Same as <code>output.good.backoffPolicy.minBackoff</code> for bad rows</td></tr><tr><td><code>output.bad.backoffPolicy.maxBackoff</code></td><td>Same as <code>output.good.backoffPolicy.maxBackoff</code> for bad rows</td></tr><tr><td><code>output.bad.backoffPolicy.maxRetries</code></td><td>Same as <code>output.good.backoffPolicy.maxRetries</code> for bad rows</td></tr><tr><td><code>output.bad.recordLimit</code></td><td>Same as <code>output.good.recordLimit</code> for bad rows</td></tr><tr><td><code><code>output.bad.customEndpoint</code></code></td><td>Same as <code>output.good.customEndpoint</code> for pii events</td></tr></tbody></table>

## Common parameters

<table class="has-fixed-layout"><tbody><tr><td><code>concurrency.enrich</code></td><td>Optional. Number of events that can get enriched at the same time within a chunk (events are processed by chunks in the app).<br/>Default: <code>256</code></td></tr><tr><td><code>concurrency.sink</code></td><td>Optional. Number of chunks that can get sunk at the same time.<br/>Default for enrich-pubsub: <code>3</code><br/>Default for enrich-kinesis: 1 (WARNING for enrich-kinesis: if greater than 1, records can get checkpointed before they are sunk)</td></tr><tr><td><code>assetsUpdatePeriod</code></td><td>Optional. Period after which enrich assets (e.g. the maxmind database for the <a href="https://docs.snowplowanalytics.com/docs/enriching-your-data/available-enrichments/ip-lookup-enrichment/">IpLookups enrichment</a>) should be checked for udpates. Assets will never be updated if this key is missing.<br/>Example: <code>7 days</code></td></tr><tr><td><code>monitoring.sentry.dsn</code></td><td>Optional. To track runtime exceptions in <a href="https://sentry.io/">Sentry</a>.<br/>Example: <code>http://sentry.acme.com</code></td></tr><tr><td><code>monitoring.metrics.statsd.hostname</code></td><td>Optional. Hostname of the <a href="https://github.com/statsd/statsd">StatsD server</a> to send enrichment metrics (latency and event counts) to.<br/>Example: <code>localhost</code></td></tr><tr><td><code>monitoring.metrics.statsd.port</code></td><td>Optional. Port of the StatsD server.<br/>Example: <code>8125</code></td></tr><tr><td><code>monitoring.metrics.statsd.period</code></td><td>Optional. How frequently to report metric<br/>Example: <code>10 seconds</code></td></tr><tr><td><code>monitoring.metrics.statsd.tags</code></td><td>Optional. Key-value pairs attached to each metric sent to StatsD to provide contextual information.<br/>Example" <code>{ "env": "prod" }</code></td></tr><tr><td><code>monitoring.metrics.statsd.prefix</code></td><td>Optional. Configures the prefix of StatsD metric names.<br/>Default: <code>snowplow.enrich.</code></td></tr><tr><td><code>monitoring.metrics.stdout.period</code></td><td>Optional. If set, metrics will be printed in the logs with this frequence.<br/>Example: <code>10 seconds</code></td></tr><tr><td><code>monitoring.metrics.stdout.prefix</code></td><td>Optional. Prefix for the metrics appearing in the logs.<br/>Default: <code>snowplow.enrich.</code></td></tr><tr><td><code>telemetry.disable</code></td><td>Optional. Set to <code>true</code> to disable telemetry</td></tr><tr><td><code>telemetry.interval</code></td><td>Optional. Interval for the heartbeat event<br/>Example: <code>1 hour</code></td></tr><tr><td><code>telemetry.method</code></td><td>Optional. HTTP method used to send the heartbeat event.<br/>Possible values: <code>GET</code> or <code>POST</code></td></tr><tr><td><code>telemetry.collectorUri</code></td><td>Optional. hostname of the collector receiving the heartbeat event.<br/>Default: <code>collector-g.snowplowanalytics.com</code></td></tr><tr><td><code>telemetry.collectorPort</code></td><td>Optional. Port of the collector receiving the heartbeat event.<br/>Example: <code>443</code></td></tr><tr><td><code>telemetry.secure</code></td><td>Optional. Whether to use https or not.<br/>Default: <code>true</code></td></tr><tr><td><code>telemetry.userProvidedId</code></td><td>Optional. Identifier intended to tie events together across modules, infrastructure and apps when used consistently.<br/>Example: <code>my_company</code></td></tr><tr><td><code>telemetry.autoGeneratedId</code></td><td>Optional. Intended to identify each independent module, and the infrastructure it controls</td></tr><tr><td><code>telemetry.instanceId</code></td><td>Optional. Unique for each instance of the app running within a module</td></tr><tr><td><code>telemetry.moduleName</code></td><td>Optional. Name of the terraform module that deployed the app.<br/>Example: <code>enrich-kinesis-ce</code></td></tr><tr><td><code>telemetry.moduleVersion</code></td><td>Optional. Version of the terraform module that deployed the app.<br/>Example: <code>1.0.0</code></td></tr><tr><td><code>featureFlags.acceptInvalid</code></td><td>Optional. Enrich 3.0.0 introduces the validation of the enriched events against <a href="https://github.com/snowplow/iglu-central/blob/master/schemas/com.snowplowanalytics.snowplow/atomic/jsonschema/1-0-0" target="_blank" rel="noreferrer noopener">atomic</a> schema before emitting. If set to false, a bad row will be emitted instead of the enriched event if validation fails. If set to true, invalid enriched events will be emitted, as before. More details <a href="https://github.com/snowplow/enrich/issues/517#issuecomment-1033910690">here</a>.<br/>Default: <code>false</code></td></tr><tr><td><code>featureFlags.legacyEnrichmentOrder</code></td><td>Optional. In early versions of enrich-kinesis and enrich-pubsub (&lt;= 3.1.5), the Javascript enrichment incorrectly ran before the currency, weather, and IP Lookups enrichments.<br/>Set this flag to <em>true</em> to keep the erroneous behaviour of those previous versions.<br/>Default: <em>false</em></td></tr></tbody></table>

Instead of Kinesis or PubSub, it's also possible to read collector payloads from files on disk. This can be used for instance for testing purposes. In this case the configuration needs to be as below.

<table class="has-fixed-layout"><tbody><tr><td><code>input.type</code></td><td>Should be <code>FileSystem</code></td></tr><tr><td><code>input.dir</code></td><td>Directory containing collector payloads encoded with Thrift</td></tr></tbody></table>

Likewise, it's possible to write enriched events, pii events and bad rows to files instead of PubSub or Kinesis.

To write enriched events to files:

<table class="has-fixed-layout"><tbody><tr><td><code>output.good.type</code></td><td>Should be <code>FileSystem</code></td></tr><tr><td><code>output.good.file</code></td><td>File where enriched events will be written</td></tr><tr><td><code>output.good.maxBytes</code></td><td>Optional. Maximum size of a file in bytes. Triggers file rotation.</td></tr></tbody></table>

To write pii events to files:

<table class="has-fixed-layout"><tbody><tr><td><code>output.pii.type</code></td><td>Should be <code>FileSystem</code></td></tr><tr><td><code>output.pii.file</code></td><td>File where pii events will be written</td></tr><tr><td><code>output.pii.maxBytes</code></td><td>Optional. Maximum size of a file in bytes. Triggers file rotation</td></tr></tbody></table>

To write bad rows to files:

<table class="has-fixed-layout"><tbody><tr><td><code>output.bad.type</code></td><td>Should be <code>FileSystem</code></td></tr><tr><td><code>output.bad.file</code></td><td>File where bad rows will be written</td></tr><tr><td><code>output.bad.maxBytes</code></td><td>Optional. Maximum size of a file in bytes. Triggers file rotation</td></tr></tbody></table>

## Enriched events validation against atomic schema

Enriched events are expected to match [atomic](https://github.com/snowplow/iglu-central/blob/master/schemas/com.snowplowanalytics.snowplow/atomic/jsonschema/1-0-0) schema.  
However, until 3.0.0, it was never checked that the enriched events emitted by enrich were valid.  
If an event is not valid against `atomic` schema, a bad row should be emitted instead of the enriched event.  
However, this is a breaking change, and we want to give some time to users to adapt, in case today they are working downstream with enriched events that are not valid against `atomic`.  
For this reason, this new validation was added as a feature that can be deactivated like that:

```
"featureFlags": {
  "acceptInvalid": true
}
```

In this case, enriched events that are not valid against `atomic` schema will still be emitted as before, so that enrich 3.0.0 can be fully backward compatible.  
It will be possible to know if the new validation would have had an impact by 2 ways:

1. A new metric `invalid_enriched` has been introducred.  
    It reports the number of enriched events that were not valid against `atomic` schema. As the other metrics, it can be seen on stdout and/or StatsD.
2. Each time there is an enriched event invalid against `atomic` schema, a line will be logged with the bad row (add `-Dorg.slf4j.simpleLogger.log.InvalidEnriched=debug` to the `JAVA_OPTS` to see it).

If `acceptInvalid` is set to `false`, a bad row will be emitted instead of the enriched event in case it's not valid against `atomic` schema.

When we'll know that all our customers don't have any invalid enriched events any more, we'll remove the feature flags and it will be impossible to emit invalid enriched events.

## Telemetry

Starting with version 3.0.0 of enrich, snowplow will be collecting the heartbeats with some meta-information about the application (schema [here](https://raw.githubusercontent.com/snowplow/iglu-central/master/schemas/com.snowplowanalytics.oss/oss_context/jsonschema/1-0-1)). This is done to help us to improve the product, we need to understand what is popular, so we could focus our development effort in the right place.

At the base, telemetry is sending the application name and version every hour. You can help us by providing `userProvidedId` in the config file :

```
"telemetry" {
  "userProvidedId": "myCompany"
}
```

Telemetry can be deactivated by putting the following section in the configuration file :

```
"telemetry": {
  "disable": true
}
```

## Enrichments

The list of the enrichments that can be configured can be found on [this page](https://docs.snowplowanalytics.com/docs/enriching-your-data/available-enrichments/).
